from igann import IGANN
import torch
import numpy as np
from igann.igann import ELM_Regressor
from scipy.interpolate import CubicSpline
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings('ignore')

class ModelAdapter():
    """
    A high-level adapter class for interfacing with various machine learning models.
    It allows for fitting, adapting, and plotting functionalities across different models
    by utilizing object composition.

    Attributes:
        model_name (str): The name of the model to use.
        model (object): The actual model instance being used.
    """

    def __init__(self, task, model="IGANN",  *args, **kwargs):
        """
        Initializes the ModelAdapter with a specified task and model.
        """
        self.model_name = model
        if self.model_name == "IGANN":
            self.model = IGANNAdapter(task=task,  *args, **kwargs)

    def fit(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict(self, X):
        return self.model.predict(X)

    def adapt(self, features_to_change, updated_data, method, hyperparamethers=None):
        """
        Adapts the model based on the specified method, which could be feature retraining or spline interpolation.
        Further approaches can also be inserted in this method.
        Parameters:
            features_to_change (list): The features to be adapted.
            updated_data (dict): The updated data (generated by the adjusted shape functions) to be used for adaptation.
            method (str): The adaptation method ('feature_retraining' or 'spline_interpolation').
            hyperparameters (tuple, optional): Additional hyperparameters required for the feature retraining method.
        """
        if self.model_name == "IGANN":
            if method == "feature_retraining":
                self.model.feature_retraining(features_to_change, updated_data, hyperparamethers=hyperparamethers)
            if method == "spline_interpolation":
                self.model.spline_interpolation(features_to_change, updated_data, hyperparamethers=hyperparamethers)
        return self.model

    # The subsequent methods or decorators are required for the frontend.
    def plot_single(self, plot_by_list=None, show_n=5):
        # Check if the underlying model has 'plot_single' method
        if hasattr(self.model, 'plot_single'):
            return self.model.plot_single(plot_by_list=plot_by_list, show_n=show_n)
        else:
            raise NotImplementedError("Plotting is not supported for this model.")

    @property
    def unique(self):
        if self.model_name == "IGANN" and hasattr(self.model, 'unique'):
            return self.model.unique
        else:
            raise AttributeError("Unique attribute not available for this model")

    @property
    def feature_names(self):
        if self.model_name == "IGANN" and hasattr(self.model, 'feature_names'):
            return self.model.feature_names
        else:
            raise AttributeError("Feature names attribute not available for this model")


    def get_shape_functions_as_dict(self):
        if self.model_name == "IGANN":
            return self.model.get_shape_functions_as_dict()




class IGANNAdapter(IGANN):
    def __init__(self, *args, **kwargs):
        super(IGANNAdapter, self).__init__(*args, **kwargs)

    def encode_categorical_data(self, features_to_change, updated_data):
        """
        Encodes categorical data for features that are to be changed, based on updated data provided.

        Parameters:
            features_to_change (list): The features to be encoded.
            updated_data (dict): The updated data containing information on how to encode the features.

        Returns:
            tuple: Extended features to change and the extended updated data with encoded features.
        """
        features_to_change_extended = []
        updated_data_extended = {}
        for feature in features_to_change:
            if updated_data[feature]['datatype'] == "categorical":
                for key, value in self.dummy_encodings.items():
                    pref_feature_name, category = key.split('_')
                    if value == feature:
                        features_to_change_extended.append(key)
                        feature_data = updated_data[feature]
                        # Find the index of the category in the 'x' list
                        if category in feature_data['x']:
                            category_index = feature_data['x'].index(category)
                            # Extract the corresponding value from the 'y' array
                            y_value = feature_data['y'][category_index]
                            updated_data_extended[key] = {
                                'x': [1],
                                'y': np.array([y_value]),
                                'datatype': feature_data['datatype']
                            }
            else:
                features_to_change_extended.append(feature)
                updated_data_extended[feature] = updated_data[feature]
        return features_to_change_extended, updated_data_extended

    def create_synthetic_data(self, nr_synthetic_data_points, features_to_change, updated_data):
        """
        Creates synthetic data to augment the training set for feature retraining.

        Parameters:
            nr_synthetic_data_points (int): The number of synthetic data points to create between each pair of original data points.
            features_to_change (list): The features for which synthetic data is to be created.
            updated_data (dict): The updated data based on which synthetic data is created.

        Returns:
            dict: The updated data dictionary including synthetic data points.
        """
        for feature in features_to_change:
            new_x_values = []
            new_y_values = []
            x_values = updated_data[feature]['x']
            y_values = updated_data[feature]['y']
            for i in range(len(x_values) - 1):
                new_x_values.append(x_values[i])
                new_y_values.append(y_values[i])
                # Calculate steps for synthetic points
                x_step = (x_values[i + 1] - x_values[i]) / (nr_synthetic_data_points + 1)
                y_step = (y_values[i + 1] - y_values[i]) / (nr_synthetic_data_points + 1)
                for j in range(1, nr_synthetic_data_points + 1):
                    synthetic_x = x_values[i] + j * x_step
                    synthetic_y = y_values[i] + j * y_step
                    new_x_values.append(synthetic_x)
                    new_y_values.append(synthetic_y)

            new_x_values.append(x_values[-1])
            new_y_values.append(y_values[-1])
            updated_data[feature]['x'] = new_x_values
            updated_data[feature]['y'] = new_y_values
        return updated_data

    def feature_retraining(self, features_to_change, updated_data, hyperparamethers=None):
        """
        Adapts the model by retraining specific features with updated data.

        Parameters:
            features_to_change (list): A list of feature names that are to be updated or retrained.
            updated_data (dict): A dictionary containing the new data for each feature to be updated. Each entry
                                 should include 'x' (features) and 'y' (target values) along with the 'datatype'.
                                 This dictionary is created in the backend based on the user's adjustments to
                                 the shape function.
            hyperparameters (tuple, optional): Additional parameters including elm_scale, elm_alpha, and
                                                nr_synthetic_data_points.
                                                If not provided, default values from the model are used.
        """
        # Extract hyperparameters if provided, otherwise use default values.
        if hyperparamethers is not None:
            elm_scale = hyperparamethers[0]
            elm_alpha = hyperparamethers[1]
            nr_synthetic_data_points = hyperparamethers[2]
            if nr_synthetic_data_points > 0:
                updated_data = self.create_synthetic_data(nr_synthetic_data_points, features_to_change, updated_data)
        else:
            elm_scale = self.elm_scale
            elm_alpha = self.elm_alpha
        # Encode categorical data and prepare it for retraining.
        features_to_change, updated_data = self.encode_categorical_data(features_to_change, updated_data)
        # Iterate through features to retrain and retrain the regressors.
        for feature in features_to_change:
            i = self.feature_names.index(feature)
            # x and y are based on the adjusted shape functions
            x = torch.tensor(updated_data[feature]['x'], dtype=torch.float32)
            y = torch.tensor(updated_data[feature]['y'], dtype=torch.float32)
            # Initial predicitions of the linear classifier
            if self.task == "classification":
                y_hat = torch.tensor(self.init_classifier.coef_[0, i] * x.numpy(), dtype=torch.float64)
            else:
                y_hat = self.init_classifier.coef_[i] * x.numpy()
            n_categorical_cols = 1 if updated_data[feature]['datatype'] == 'categorical' else 0
            # Iterate through the nr. of non-linear base functions / regressors
            for counter, regressor in enumerate(self.regressors):
                if updated_data[feature]['datatype'] == 'numerical':
                    # Get y_tilde
                    if self.task == "classification":
                        y_tilde = torch.sqrt(torch.tensor(0.5)) * (torch.tensor((y - y_hat), dtype=torch.float64))
                    else:
                        y_tilde = torch.sqrt(torch.tensor(0.5).to(self.device)) * self._get_y_tilde(y, y_hat).to(dtype=torch.float32)

                    hessian_train_sqrt = self._loss_sqrt_hessian(y, y_hat)
                    # Instantiate new regressor with hyperparamters
                    new_regressor = ELM_Regressor(
                        n_input=1,
                        n_categorical_cols=n_categorical_cols,
                        n_hid=self.n_hid,
                        seed=0,
                        elm_scale=elm_scale,
                        elm_alpha=elm_alpha,
                        act=self.act,
                        device=self.device,
                    )
                    # For some datasets and user adjustments (in particular in classification tasks),
                    # the model did not approximate the adjustments well enough. Therefore the additional
                    # regularization parameter mult_coef is removed for classification tasks by setting it to 1.
                    mult_coef = torch.sqrt(torch.tensor(0.5).to(self.device))* self.boost_rate * hessian_train_sqrt[:, None]

                    if self.task == 'classification':
                        mult_coef = torch.ones_like(mult_coef)

                    # Fit the ELM regressor
                    X_hid = new_regressor.fit(
                        x.reshape(-1, 1),
                        y_tilde,
                        mult_coef
                    )

                    # Make a prediction of the ELM for the update of y_hat
                    if self.task == "classification":
                        train_regressor_pred = new_regressor.predict(X_hid.to(dtype=torch.float64), hidden=True).squeeze()
                    else:
                        train_regressor_pred = new_regressor.predict(X_hid, hidden=True).squeeze()
                    # Update the prediction for training data
                    y_hat = torch.tensor(y_hat, dtype=torch.float64)
                    y_hat += self.boost_rate * train_regressor_pred
                    y_hat = self._clip_p(y_hat)
                    # Replace the weights in the list of original regressors
                    regressor.hidden_mat[i, i * self.n_hid: (i + 1) * self.n_hid] = new_regressor.hidden_mat.squeeze()
                    regressor.output_model.coef_[
                    i * self.n_hid: (i + 1) * self.n_hid] = new_regressor.output_model.coef_
                else:
                    # For categorical features update the weight directly (no training process required)
                    share_of_init_classifier = np.array(y_hat / len(self.regressors))
                    new_weight = (y / len(self.regressors) - share_of_init_classifier) * (1 / self.boosting_rates[counter])
                    regressor.output_model.coef_[i * self.n_hid: (i + 1) * self.n_hid] = new_weight


    def spline_interpolation(self, features_to_change, updated_data, hyperparamethers=None):
        """
        Adapts the model by applying cubic spline interpolation to the specified features.
        For the specified features, the original ELM_Regressors are replaced with new ones,
        containing the same information as the old ones, but altered prediction methods.
        These altered prediction methods set the output of the features_to_change to 0 and
        instead add the estimate of the spline interpolation of the user-adapted shape functions.

        Parameters:
            features_to_change (list): A list of feature where the shape functions got changed.
            updated_data (dict): A dictionary with the updated feature data. Each feature's data should include
                                 'x' values (feature points) and 'y' values (target points) for interpolation.
            hyperparameters (None, optional): Currently not used but included for consistency with the interface.
        """
        features_to_change, updated_data = self.encode_categorical_data(features_to_change, updated_data)
        # Create cubic spline functions or direct mappings for numerical and categorical features, respectively.
        spline_functions = {}
        features_selected_i = []
        for feature in features_to_change:
            feature_index = self.feature_names.index(feature)
            x_data = updated_data[feature]['x']
            y_data = updated_data[feature]['y']
            if updated_data[feature]['datatype'] == 'numerical':
                spline = CubicSpline(x_data, y_data, extrapolate=True)
            else:
                # for categorical features, spline is the y-value, that the user determined
                spline = y_data
            spline_functions[feature_index] = spline
            features_selected_i.append(feature_index)

        # Copy ELM Regressor
        new_regressors = []
        # Create custom Regressor
        for index, regressor in enumerate(self.regressors):
            new_regressor = ELM_Regressor_Spline(
                n_input=regressor.n_input,
                n_categorical_cols=regressor.n_categorical_cols,
                n_hid=regressor.n_hid,
                # seed is not an instance attribute in IGANN code
                seed=getattr(regressor, 'seed', 0),
                elm_scale=regressor.elm_scale,
                elm_alpha=regressor.elm_alpha,
                act=regressor.act,
                device=regressor.device,
                features_selected_i=features_selected_i,
                spline_functions=spline_functions,
                n_regressors=len(self.regressors),
                boosting_rates=self.boosting_rates,
                index_regressor=index,
                init_classifier=self.init_classifier,
                task=self.task
            )
            # Copying all attributes from the old regressor to the new regressor
            for attr in vars(regressor):
                setattr(new_regressor, attr, getattr(regressor, attr))
            new_regressor.spline_functions = spline_functions
            new_regressors.append(new_regressor)
        self.regressors = new_regressors





class ELM_Regressor_Spline(ELM_Regressor):
    """
    A specialized version of the ELM_Regressor that incorporates spline functions for selected features.

    Inherits from ELM_Regressor.

    Attributes:
        features_selected_i (list): Indices of the selected features for spline interpolation.
        spline_functions (dict): Spline functions for the selected features.
        n_regressors (int): Number of regressors in the ensemble.
        boosting_rates (list): Boosting rates for each regressor in the ensemble.
        index_regressor (int): The index of the current regressor.
        init_classifier (object): The initial classifier used.
        task (str): The task type ('classification' or 'regression').
    """
    def __init__(self, features_selected_i, spline_functions, n_regressors,
                 boosting_rates, index_regressor, init_classifier, task, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.features_selected_i = features_selected_i
        self.spline_functions = spline_functions
        self.n_regressors = n_regressors
        self.boosting_rates = boosting_rates
        self.index_regressor = index_regressor
        self.init_classifier = init_classifier
        self.task = task

    def predict(self, X, hidden=False):
        """
        This function makes a full prediction with the model for a given input X.
        """
        if hidden:
            X_hid = X
        else:
            X_hid = self.get_hidden_values(X)

        # Set the coefficients for selected features to 0
        for i in self.features_selected_i:
            if i < self.n_numerical_cols:
                # numerical feature
                self.output_model.coef_[i * self.n_hid: (i + 1) * self.n_hid] = 0
            else:
                start_idx = self.n_numerical_cols * self.n_hid + (i - self.n_numerical_cols)
                self.output_model.coef_[start_idx: start_idx + 1] = 0
        # Use the modified coefficients for prediction
        out = X_hid @ self.output_model.coef_
        # Add spline function values for selected features
        for i in self.features_selected_i:
            spline = self.spline_functions[i]
            if self.task == "classification":
                pred_init_classifier = self.init_classifier.coef_[0, i] * X[:, i]
            else:
                pred_init_classifier = self.init_classifier.coef_[i] * X[:, i]
            share_of_init_classifier = np.array(pred_init_classifier / self.n_regressors)
            if i < self.n_numerical_cols:
                prediction = ((spline(X[:, i]) / self.n_regressors) - share_of_init_classifier) * (1/self.boosting_rates[self.index_regressor])
            else:
                # for categorical features, spline is simply the y value, which the user determined
                user_prediction = torch.from_numpy(self.spline_functions[i]) * X[:, i]
                prediction = ((user_prediction / self.n_regressors) - share_of_init_classifier) * (1 / self.boosting_rates[self.index_regressor])
            out += prediction
        return out

    def predict_single(self, x, i):
        """
        This function computes the partial output of one base function for one feature.
        # See self.predict for the description - it's almost equivalent.
        """
        x_in = x.reshape(len(x), 1)
        if i not in self.features_selected_i:
            # normal prediction
            x_in = x.reshape(len(x), 1)
            if i < self.n_numerical_cols:
                # numerical feature
                x_in = x_in @ self.hidden_mat[
                              i, i * self.n_hid: (i + 1) * self.n_hid
                              ].unsqueeze(0)
                x_in = self.act(x_in)
                out = x_in @ self.output_model.coef_[
                             i * self.n_hid: (i + 1) * self.n_hid
                             ].unsqueeze(1)
            else:
                # categorical feature
                start_idx = self.n_numerical_cols * self.n_hid + (i - self.n_numerical_cols)
                out = x_in @ self.output_model.coef_[start_idx: start_idx + 1].unsqueeze(1)
            return out
        else:
            if self.task == "classification":
                pred_init_classifier = self.init_classifier.coef_[0, i] * x
            else:
                pred_init_classifier = self.init_classifier.coef_[i] * x
            share_of_init_classifier = np.array(pred_init_classifier / self.n_regressors)
            if i < self.n_numerical_cols:
                # If the feature is selected, use the spline function for prediction
                spline = self.spline_functions[i]
                out = ((spline(x) / self.n_regressors) - share_of_init_classifier) * (1/self.boosting_rates[self.index_regressor])
                if isinstance(out, np.ndarray):
                    out = torch.from_numpy(out).float()
            else:
                prediction = torch.from_numpy(self.spline_functions[i]) * x
                out = ((prediction / self.n_regressors)- share_of_init_classifier) * (1/self.boosting_rates[self.index_regressor])
                if isinstance(out, np.ndarray):
                    out = torch.from_numpy(out).float()
            return out.squeeze()

